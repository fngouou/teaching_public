# Deep Natural Language Processing

## Table of contents

- Course 1
  - [Lesson](./lesson_01.md)
    - Course introduction
    - General machine learning terminology
    - Deep learning history
    - Linear regression example
    - Loss function
    - Gradient descent (with its stochastic variation)
  - [Practical work](./practical_work_01.md)
    - Anaconda environment setup
    - Manipulation of PyTorch tensors
      - Creation
      - Indexing
      - Slicing
      - Shape manipulation
      - Combination
      - Aggregation
      - Broadcasting
      - Boolean logic and indexing
- Course 2
  - [Lesson](./lesson_02.md)
    - Neuron definition
    - Neural network definition
    - Softmax activation function
    - Cross entropy loss
    - Neural network example
  - [Practical work](./practical_work_02.md)
    - MNIST image classification using Multi-Layer Perceptron
    - Model evaluation
- Course 3
  - [Lesson](./lesson_03.md)
    - Backpropagation algorithm
    - Convolution layer
    - Pooling layer
    - Convolutional neural network example
  - [Practical work](./practical_work_03.md)
    - MNIST image classification using Convolutional neural network
    - Optimizer change experimentation
    - CIFAR-10 image classification using Convolutional neural network
- Course 4
  - [Lesson](./lesson_04.md)
    - CIFAR-10 Convolutional neural network solution
    - Dropout layer
    - Modular design of neural networks
    - Learning rate decay
    - Data augmentation
    - Concept of Transfer learning
  - [Practical work](./practical_work_04.md)
    - CIFAR-10 using modular design, dropout, data augmentation and
      learning rate decay
    - Resnet transfer learning
- Course 5
  - [Lesson](./lesson_05.md)
    - Transfer learning in practice
    - Neural networks for Natural Language Processing
      - Word embeddings
      - Text preprocessing methodology
      - Multi-layer perceptron text classifier
      - 1D CNN text classifier
      - RNN text classifier
  - [Practical work](./practical_work_05.md)
    - Multi-layer perceptron image autoencoder
    - Convolutional image autoencoder
- Course 6
  - [Lesson](./lesson_06.md)
    - Transfer learning for NLP
      - Language modeling
      - Next sentence prediction
    - BERT
      - Attention mechanism
      - BERT embeddings
      - Types of tasks BERT can handle
      - BERT architecture
      - BERT finetuning example on sentiment classification
  - [Practical work](./practical_work_06.md)
    - BERT application to horoscope classification
    - Horoscope language modeling

## References

- [Google machine learning crash course](https://developers.google.com/machine-learning/crash-course)
- [Python Data Science Handbook](https://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb) by Jake VanderPlas
- [PyTorch tutorials](https://pytorch.org/tutorials/)
- [Deep Learning, NLP, and
 Representations](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)
 by Christopher Olah
- [Oxford Deep NLP course](https://github.com/oxford-cs-deepnlp-2017/lectures)
- [University of Illinois CS 521: Statistical Natural Language
  Processing](http://www.natalieparde.com/teaching/cs521_spring2020.html)
- [Stanford CS224n: Natural Language Processing with Deep
Learning](http://web.stanford.edu/class/cs224n/)
- [Carnegie Mellon University CS11-747: Neural networks for
  NLP](http://phontron.com/class/nn4nlp2020/index.html)
